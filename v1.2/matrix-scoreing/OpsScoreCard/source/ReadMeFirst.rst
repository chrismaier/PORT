
About Scoring - Read Me First
=============================

============
How To Score
============

 
Keep the objectives in mind when scoring.  A few highlights regarding the primary goals of the PORT:

* Ensure that Rackspace software and systems are easy to maintain in a manner that causes zero impact to our customers.
* Reduce the likelihood of an unplanned outage of any type
* Simplify the manner in which systems are supported
* Reduce TCO of all systems
* Deliver an all around Fanatical experience for our customers

Rackspace PORT leadership has designed the scoring to be considered as "floor value".  If your application or system does not meet or exceed ALL of the items in the category being scored, then you must take the lower value.  For example, if your monitoring is what you think qualifies for a level 3 but you don't have a dashboard set up and the log output is not "manager accessible", then the score would be a level 2, not a level 3 for the logging section.  Same pattern and methodology holds true for all categories.  This also means there are no half scores or "point five" scores.  There is no level 4, there is level 3 and then level 5.  There is no level 2.5, or "almost level 3".
 
All categories and items are in scope.  For example, if your application is dependent on a downstream component or system that doesn't meet the multi-data-center data replication requirements, then your system will quite likely never be able to exceed the level 2 for the recover-ability section.  The rating is for the entire system and the ability for the customer to access the system at all times and to be able to do so without losing data or suffering feature impact and the like.  All dependent systems are therefore in-scope and the rating is not just for your particular component or portion of the system.
 
Scoring is to be completed by multiple individuals from multiple teams.  It is not acceptable to have a single project/product manager score the system.  Developers (not just the developer manager), and operations folks, along with the product manager, and one or two independent third parties should all independently score the system and then meet to discuss the results.  The post scoring meeting should be the place where any discrepancies are discussed and justifications are presented.  A collaborative score should then be agreed upon.
 
Remember that the scoring will change with time.  The industry at large and our competitors are constantly advancing, therefore so should our systems.  The set of standards used to score a system year one may not be the same set of standards on year two - the port requirements associated with Level 1, Level 2, Level 3, etc. will evolve over time to reflect industry norms and what Rackspace defines as Fanatical.

